main_config:
  batch_size: 10
  model_name: "Alibaba-NLP/gte-large-en-v1.5"
  # model_name: "nlpaueb/legal-bert-base-uncased"

Alibaba-NLP/gte-large-en-v1.5:
  context_length: 8192
  chunk_size: 4000
  chunk_overlap: 200
  embedding_size: 1024
  model_type: "hugging_face"

all-MiniLM-L6-v2:
  context_length: 256
  chunk_size: 1000
  chunk_overlap: 200
  model_type: "sentence_transformer"

intfloat/e5-mistral-7b-instruct:
  context_length: 32768
  chunk_size: 25000
  chunk_overlap: 400
  embedding_size: 4096

nlpaueb/legal-bert-base-uncased:
  context_length: 512
  chunk_size: 512
  chunk_overlap: 100
  embedding_size: 768
  model_type: "hugging_face"
